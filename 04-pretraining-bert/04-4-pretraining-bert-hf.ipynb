{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-4: Pre-training BERT Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab only:\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install datasets evaluate \n",
    "    exit() # Restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Transformers version : {datasets.__version__}\")\n",
    "print(f\"Datasets version : {transformers.__version__}\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n",
    "TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n",
    "\n",
    "BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n",
    "NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n",
    "SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n",
    "\n",
    "MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n",
    "\n",
    "TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n",
    "LEARNING_RATE = 1e-4  # Learning rate for training the model\n",
    "\n",
    "MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from ðŸ¤— Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the WikiText dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# set ignore_verification=True\n",
    "# https://github.com/huggingface/datasets/issues/2969\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", ignore_verifications=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a new Tokenizer\n",
    "all_texts = [\n",
    "    doc for doc in dataset[\"train\"][\"text\"] if len(doc) > 0 and not doc.startswith(\" =\")\n",
    "]\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(all_texts), TOKENIZER_BATCH_SIZE):\n",
    "        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=TOKENIZER_VOCABULARY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Pre-processing\n",
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(1000)])\n",
    "\n",
    "# We define the maximum number of tokens after tokenization that each training sample\n",
    "# will have\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    \"\"\"Function to prepare features for NSP task\n",
    "\n",
    "    Arguments:\n",
    "      examples: A dictionary with 1 key (\"text\")\n",
    "        text: List of raw documents (str)\n",
    "    Returns:\n",
    "      examples:  A dictionary with 4 keys\n",
    "        input_ids: List of tokenized, concatnated, and batched\n",
    "          sentences from the individual raw documents (int)\n",
    "        token_type_ids: List of integers (0 or 1) corresponding\n",
    "          to: 0 for senetence no. 1 and padding, 1 for sentence\n",
    "          no. 2\n",
    "        attention_mask: List of integers (0 or 1) corresponding\n",
    "          to: 1 for non-padded tokens, 0 for padded\n",
    "        next_sentence_label: List of integers (0 or 1) corresponding\n",
    "          to: 1 if the second sentence actually follows the first,\n",
    "          0 if the senetence is sampled from somewhere else in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove un-wanted samples from the training set\n",
    "    examples[\"document\"] = [\n",
    "        d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")\n",
    "    ]\n",
    "    # Split the documents from the dataset into it's individual sentences\n",
    "    examples[\"sentences\"] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n",
    "    ]\n",
    "    # Convert the tokens into ids using the trained tokenizer\n",
    "    examples[\"tokenized_sentences\"] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n",
    "        for doc in examples[\"sentences\"]\n",
    "    ]\n",
    "\n",
    "    # Define the outputs\n",
    "    examples[\"input_ids\"] = []\n",
    "    examples[\"token_type_ids\"] = []\n",
    "    examples[\"attention_mask\"] = []\n",
    "    examples[\"next_sentence_label\"] = []\n",
    "\n",
    "    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "        target_seq_length = max_num_tokens\n",
    "\n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    tokens_b = []\n",
    "\n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(\n",
    "                                0, len(examples[\"tokenized_sentences\"]) - 1\n",
    "                            )\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "\n",
    "                        random_document = examples[\"tokenized_sentences\"][\n",
    "                            random_document_index\n",
    "                        ]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    input_ids = tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "\n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n",
    "                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n",
    "                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n",
    "                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            i += 1\n",
    "\n",
    "    # We delete all the un-necessary columns from our dataset\n",
    "    del examples[\"document\"]\n",
    "    del examples[\"sentences\"]\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"tokenized_sentences\"]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "# quiet output for dataset.map\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, batched=True, remove_columns=[\"text\"], num_proc=1,\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create Masked Language Model\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.25\n",
    ")\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate  # from datasets import load_metric is deprecated since datasets==3.0.0\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForPreTraining\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_bert_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    eval_strategy = 'steps',\n",
    "    report_to=\"none\" # None does not work with wandb\n",
    ")\n",
    "\n",
    "\n",
    "# Load the pretrained BERT model\n",
    "model = AutoModelForPreTraining.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# To see some metrics (commented to avoid timeout)\n",
    "#print(trainer.evaluate(tokenized_dataset['test']))\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./my_bert_pretrained\")\n",
    "\n",
    "logging.info(\"Saving model locally ....\")\n",
    "model.save_pretrained('my_model_output', saved_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
