{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-3: Pre-training BERT with `keras_nlp.models.BertBackbone`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install -q --upgrade keras  # Upgrade to Keras 3.\n",
    "\n",
    "# temporal workaround to solve dependency issues between Keras 3 and tensorflow-hub\n",
    "# https://github.com/keras-team/keras-nlp/issues/1417\n",
    "#!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://storage.googleapis.com/miax/nlp/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!# Remove unsupervised examples\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "imdb_train = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "imdb_test = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Inspect first review\n",
    "# Format is (review text tensor, label tensor)\n",
    "print(imdb_train.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All BERT `en` models have the same vocabulary, so reuse preprocessor from\n",
    "# \"bert_tiny_en_uncased\"\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_tiny_en_uncased\",\n",
    "    sequence_length=256,\n",
    ")\n",
    "packer = preprocessor.packer\n",
    "tokenizer = preprocessor.tokenizer\n",
    "\n",
    "# keras.Layer to replace some input tokens with the \"[MASK]\" token\n",
    "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=0.25,\n",
    "    mask_selection_length=64,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    "    unselectable_token_ids=[\n",
    "        tokenizer.token_to_id(x) for x in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs, label):\n",
    "    inputs = preprocessor(inputs)\n",
    "    masked_inputs = masker(inputs[\"token_ids\"])\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"token_ids\": masked_inputs[\"token_ids\"],\n",
    "        \"segment_ids\": inputs[\"segment_ids\"],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"],\n",
    "        \"mask_positions\": masked_inputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = masked_inputs[\"mask_ids\"]\n",
    "    weights = masked_inputs[\"mask_weights\"]\n",
    "    return features, labels, weights\n",
    "\n",
    "\n",
    "pretrain_ds = imdb_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
    "    tf.data.AUTOTUNE\n",
    ")\n",
    "pretrain_val_ds = imdb_test.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tokens with ID 103 are \"masked\"\n",
    "print(pretrain_ds.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lanzar el pre-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT backbone\n",
    "backbone = keras_nlp.models.BertBackbone(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    "    hidden_dim=128,\n",
    "    intermediate_dim=512,\n",
    ")\n",
    "\n",
    "# Language modeling head\n",
    "mlm_head = keras_nlp.layers.MaskedLMHead(\n",
    "    token_embedding=backbone.token_embedding,\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"token_ids\"),\n",
    "    \"segment_ids\": keras.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\"),\n",
    "    \"padding_mask\": keras.Input(shape=(None,), dtype=tf.int32, name=\"padding_mask\"),\n",
    "    \"mask_positions\": keras.Input(shape=(None,), dtype=tf.int32, name=\"mask_positions\"),\n",
    "}\n",
    "\n",
    "# Encoded token sequence\n",
    "sequence = backbone(inputs)[\"sequence_output\"]\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = mlm_head(sequence, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.summary()\n",
    "pretraining_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=5e-4),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")\n",
    "\n",
    "# Pretrain on IMDB dataset\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds,\n",
    "    validation_data=pretrain_val_ds,\n",
    "    epochs=6,  # Increase to 6 for higher accuracy\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
