{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-4. Comparativa OpenAI y Google y despliegue en wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb\n",
    "#!pip install openai\n",
    "#!pip install google-cloud-platform --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "config = dict(\n",
    "    temperature = 1.0,\n",
    "    max_output_tokens = 128,\n",
    "    top_p = 0.8,\n",
    "    top_k = 40,\n",
    ")\n",
    "wandb.init(project=\"comparison-openai-gemini\", config=config, name = \"comparison\")\n",
    "print(wandb.util.generate_id())\n",
    "print(wandb.run)\n",
    "table = wandb.Table(columns=[\"model\", \"test\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GCP and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE PROJECT_ID\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"   # <---- CHANGE THIS\n",
    "LOCATION = \"us-central1\"   \n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"text-bison@001\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "generative_model = GenerativeModel(\"gemini-pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# TODO: CHANGE OPENAI API KEY\n",
    "client = OpenAI(api_key=YOUR_OPENAI_API_KEY)  # <---- CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_llm(model, description, parameters, llm_call, show_activity = False):\n",
    "  \n",
    "  # text-bison\n",
    "  t0 = time.perf_counter()\n",
    "  res = model.predict(llm_call, **parameters).text\n",
    "\n",
    "  table.add_data(\n",
    "      \"text-bison@002\", \n",
    "      description, \n",
    "      time.perf_counter() - t0, \n",
    "      config[\"temperature\"], \n",
    "      config[\"max_output_tokens\"], \n",
    "      config[\"top_p\"], \n",
    "      config[\"top_k\"], \n",
    "      llm_call, \n",
    "      res)\n",
    "  \n",
    "  # gpt-3.5-turbo (ChatGPT)\n",
    "  t0 = time.perf_counter()\n",
    "  res_openai = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    \"gpt-3.5-turbo\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "  \n",
    "  # gpt-4-8k\n",
    "  t0 = time.perf_counter()\n",
    "  res_openai = client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    \"gpgpt-4-0613\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "\n",
    "  # Gemini text\n",
    "  t0 = time.perf_counter() \n",
    "  res_gen = generative_model.generate_content(\n",
    "    [llm_call],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 1\n",
    "    },\n",
    "  stream=False,\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    \"gemini-pro\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    0.9, \n",
    "    2048, \n",
    "    1, \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_gen.text\n",
    "  )\n",
    "  \n",
    "  ## Only show response from text-bison, not openai or Gemini\n",
    "  if show_activity:\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
    "    print(res)\n",
    "        \n",
    "\n",
    "  return res  # Return to `_` if not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"The planet earth is the \",\n",
    "    \"Implement a Python function to compute the Fibonacci numbers.\",\n",
    "    \"Write a Rust function that performs binary exponentiation.\",\n",
    "    \"How do I allocate memory in C?\",\n",
    "    \"What are the differences between Javascript and Python?\",\n",
    "    \"How do I find invalid indices in Postgres?\",\n",
    "    \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n",
    "    \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n",
    "    \"Can you explain how a decision tree algorithm works in machine learning?\",\n",
    "    \"How would you design a simple key-value store database from scratch?\",\n",
    "    \"How do you handle deadlock situations in concurrent programming?\",\n",
    "    \"What is the logic behind the A* search algorithm, and where is it used?\",\n",
    "    \"How can you design an efficient autocomplete system?\",\n",
    "    \"What approach would you take to design a secure session management system in a web application?\",\n",
    "    \"How would you handle collision in a hash table?\",\n",
    "    \"How can you implement a load balancer for a distributed system?\",\n",
    "    \"What is the fable involving a fox and grapes?\",\n",
    "    \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
    "    \"Who does Harry turn into a balloon?\",\n",
    "    \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
    "    \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm(queries):\n",
    "    t0 = time.perf_counter()\n",
    "    res = call_llm(model, \"comparison\", parameters, q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"comparison\": table})\n",
    "table = wandb.Table(columns=[\"model\", \"test\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
