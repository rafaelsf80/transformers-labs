{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-4. Comparativa LLMs y métricas en wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install openai\n",
    "!pip install cohere\n",
    "!pip install google-cloud-aiplatform google-genai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"comparison-llm\", name = \"comparison\")\n",
    "print(wandb.util.generate_id())\n",
    "print(wandb.run)\n",
    "table = wandb.Table(columns=[\"model\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI, Cohere and GOogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert YOUR_OPEN_API_KEY\n",
    "from openai import OpenAI\n",
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass()\n",
    "openai_client = OpenAI(api_key=openai_api_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert YOUR_COHERE_API_KEY\n",
    "import cohere\n",
    "\n",
    "cohere_api_key = getpass.getpass()\n",
    "co = cohere.ClientV2(cohere_api_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert YOUR_GOOGLE_API_KEY\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "google_api_key = getpass.getpass() \n",
    "\n",
    "google_client = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    temperature = 1.0,\n",
    "    max_output_tokens = 128,\n",
    "    top_p = 0.8,\n",
    "    top_k = 40,\n",
    ")\n",
    "\n",
    "MODEL_GEMMA        = \"gemma-3-1b-it\"\n",
    "MODEL_GOOGLE       = \"gemini-2.0-flash\"\n",
    "MODEL_COHERE       = \"command-a-03-2025\"\n",
    "MODEL_OPENAI_TOP   = \"gpt-4.1\"\n",
    "MODEL_OPENAI_BASIC = \"gpt-4-0613\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_llms(config, llm_call, show_activity = True):\n",
    "  \n",
    "  # Gemma\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  response = google_client.models.generate_content(\n",
    "    model=MODEL_GEMMA,\n",
    "    contents=llm_call,\n",
    "    config=GenerateContentConfig(\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            top_k=config[\"top_k\"],\n",
    "            candidate_count=1,  \n",
    "    )\n",
    "  )\n",
    "\n",
    "  res_gemma = response.text\n",
    "\n",
    "  table.add_data(\n",
    "       MODEL_GEMMA, \n",
    "       time.perf_counter() - t0, \n",
    "       config[\"temperature\"], \n",
    "       config[\"max_output_tokens\"], \n",
    "       config[\"top_p\"], \n",
    "       config[\"top_k\"], \n",
    "       llm_call, \n",
    "       res_gemma)\n",
    "  \n",
    "  # Gemini 2.0 flash\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  response = google_client.models.generate_content(\n",
    "    model=MODEL_GOOGLE, \n",
    "    contents=llm_call,\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config[\"top_p\"],\n",
    "        top_k=config[\"top_k\"],\n",
    "        candidate_count=1,  \n",
    "  )\n",
    ")\n",
    "  res_google = response.text\n",
    "\n",
    "  table.add_data(\n",
    "       MODEL_GOOGLE, \n",
    "       time.perf_counter() - t0, \n",
    "       config[\"temperature\"], \n",
    "       config[\"max_output_tokens\"], \n",
    "       config[\"top_p\"], \n",
    "       config[\"top_k\"], \n",
    "       llm_call, \n",
    "       res_google)\n",
    "  \n",
    "  # gpt-4.1\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  res_openai = openai_client.responses.create(\n",
    "      model=MODEL_OPENAI_TOP,\n",
    "      input=llm_call\n",
    "  )\n",
    "\n",
    "  table.add_data(\n",
    "    MODEL_OPENAI_TOP, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.output_text\n",
    "  )\n",
    "  \n",
    "  # gpt-4-0613\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  res_openai = openai_client.chat.completions.create(\n",
    "    model=MODEL_OPENAI_BASIC,\n",
    "    temperature=config[\"temperature\"],\n",
    "    top_p=config[\"top_p\"],\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    MODEL_OPENAI_BASIC, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "  \n",
    "  # Cohere\n",
    "  t0 = time.perf_counter() \n",
    "\n",
    "  res_cohere = co.chat(\n",
    "      model=MODEL_COHERE, \n",
    "      messages=[{\"role\": \"user\", \"content\": llm_call}],\n",
    "      max_tokens=config[\"max_output_tokens\"], \n",
    "      temperature=config[\"temperature\"],\n",
    "      k=config[\"top_k\"],\n",
    "      p=config[\"top_p\"]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    MODEL_COHERE, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_cohere.message.content[0].text \n",
    "  )\n",
    "    \n",
    "  ## Only show response from Gemini, not from all LLMs\n",
    "  if show_activity:\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "    print(f\"{BOLD}The call to Gemini LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
    "    print(res_google)\n",
    "        \n",
    "\n",
    "  return res_google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"El planeta Tierra es \",\n",
    "    \"Implementar una función en Python para calcular los números de Fibonacci.\",\n",
    "    \"Escribir una función en Rust que realice una exponenciación binaria.\",\n",
    "    \"¿Cómo reservo memoria en C?\",\n",
    "    # \"¿Cuáles son las diferencias entre Javascript y Python?\",\n",
    "    # \"¿Cómo encuentro índices inválidos en Postgres?\",\n",
    "    # \"¿Cómo puedes implementar una caché LRU (Least Recently Used) en Python?\",\n",
    "    # \"¿Qué enfoque usarías para detectar y prevenir condiciones de carrera en una aplicación multiproceso?\",\n",
    "    # \"¿Puedes explicar cómo funciona un algoritmo de árbol de decisión en el aprendizaje automático?\",\n",
    "    # \"¿Cómo diseñarías una base de datos simple de almacenamiento clave-valor desde cero?\",\n",
    "    # \"¿Cómo manejas situaciones de interbloqueo en la programación concurrente?\",\n",
    "    # \"¿Cuál es la lógica detrás del algoritmo de búsqueda A*, y dónde se utiliza?\",\n",
    "    # \"¿Cómo puedes diseñar un sistema de autocompletado eficiente?\",\n",
    "    # \"¿Qué enfoque tomarías para diseñar un sistema seguro de gestión de sesiones en una aplicación web?\",\n",
    "    # \"¿Cómo manejarías las colisiones en una tabla hash?\",\n",
    "    # \"¿Cómo puedes implementar un balanceador de carga para un sistema distribuido?\",\n",
    "    \"Escribe un cuento sobre un historiador que viaja en el tiempo y que presencie los eventos más significativos de la historia de la humanidad.\",\n",
    "    \"Describe un día en la vida de un agente secreto que también es padre/madre a tiempo completo.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(queries):\n",
    "    t0 = time.perf_counter()\n",
    "    res = call_llms(config, q, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"llm-comparison-180525\": table})\n",
    "table = wandb.Table(columns=[\"model\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
