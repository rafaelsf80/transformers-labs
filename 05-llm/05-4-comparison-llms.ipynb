{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-4. Comparativa LLMs y métricas en wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install openai\n",
    "!pip install cohere\n",
    "!pip install google-cloud-aiplatform google-genai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"comparison-llm\", config=config, name = \"comparison\")\n",
    "print(wandb.util.generate_id())\n",
    "print(wandb.run)\n",
    "table = wandb.Table(columns=[\"model\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI, Cohere and GOogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace YOUR_OPEN_API_KEY\n",
    "from openai import OpenAI\n",
    "openai_client = OpenAI(api_key=YOUR_OPENAI_API_KEY) # <--- CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace YOUR_COHERE_API_KEY\n",
    "import cohere\n",
    "co = cohere.ClientV2(YOUR_COHERE_API_KEY) # <--- CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace YOUR_GOOGLE_API_KEY\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from google import genai\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\" # <--- CHANGE THIS\n",
    "LOCATION = \"europe-west4\"\n",
    "\n",
    "gemini_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    temperature = 1.0,\n",
    "    max_output_tokens = 128,\n",
    "    top_p = 0.8,\n",
    "    top_k = 40,\n",
    ")\n",
    "\n",
    "MODEL_GOOGLE       = \"gemini-2.0-flash\"\n",
    "MODEL_COHERE       = \"command-a-03-2025\"\n",
    "MODEL_OPENAI_TOP   = \"gpt-4.1\"\n",
    "MODEL_OPENAI_BASIC = \"gpt-4-0613\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_llms(config, llm_call, show_activity = True):\n",
    "  \n",
    "  # Gemini 2.0 flash\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  response = gemini_client.models.generate_content(\n",
    "    model=MODEL_GOOGLE, contents=llm_call\n",
    ")\n",
    "  res_google = response.text\n",
    "\n",
    "  table.add_data(\n",
    "       MODEL_GOOGLE, \n",
    "       time.perf_counter() - t0, \n",
    "       config[\"temperature\"], \n",
    "       config[\"max_output_tokens\"], \n",
    "       config[\"top_p\"], \n",
    "       config[\"top_k\"], \n",
    "       llm_call, \n",
    "       res_google)\n",
    "  \n",
    "  # gpt-4.1\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  res_openai = openai_client.responses.create(\n",
    "      model=MODEL_OPENAI_TOP,\n",
    "      input=llm_call\n",
    "  )\n",
    "\n",
    "  table.add_data(\n",
    "    MODEL_OPENAI_TOP, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.output_text\n",
    "  )\n",
    "  \n",
    "  # gpt-4-8k\n",
    "  t0 = time.perf_counter()\n",
    "\n",
    "  res_openai = openai_client.chat.completions.create(\n",
    "    model=MODEL_OPENAI_BASIC,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    MODEL_OPENAI_BASIC, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "  \n",
    "  # Cohere\n",
    "  t0 = time.perf_counter() \n",
    "\n",
    "  res_cohere = co.chat(\n",
    "      model=MODEL_COHERE, \n",
    "      messages=[{\"role\": \"user\", \"content\": llm_call}],\n",
    "      max_tokens=config[\"max_output_tokens\"], \n",
    "      temperature=config[\"temperature\"],\n",
    "      k=config[\"top_k\"],\n",
    "      p=config[\"top_p\"]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    MODEL_COHERE, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_cohere.message.content[0].text \n",
    "  )\n",
    "    \n",
    "  ## Only show response from Gemini, not from all LLMs\n",
    "  if show_activity:\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "    print(f\"{BOLD}The call to Gemini LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
    "    print(res_google)\n",
    "        \n",
    "\n",
    "  return res_google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"The planet earth is the \",\n",
    "    \"Implement a Python function to compute the Fibonacci numbers.\",\n",
    "    \"Write a Rust function that performs binary exponentiation.\",\n",
    "    \"How do I allocate memory in C?\",\n",
    "    \"What are the differences between Javascript and Python?\",\n",
    "    \"How do I find invalid indices in Postgres?\",\n",
    "    \"How can you implement a LRU (Least Recently Used) cache in Python?\",\n",
    "    \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",\n",
    "    \"Can you explain how a decision tree algorithm works in machine learning?\",\n",
    "    \"How would you design a simple key-value store database from scratch?\",\n",
    "    \"How do you handle deadlock situations in concurrent programming?\",\n",
    "    \"What is the logic behind the A* search algorithm, and where is it used?\",\n",
    "    \"How can you design an efficient autocomplete system?\",\n",
    "    \"What approach would you take to design a secure session management system in a web application?\",\n",
    "    \"How would you handle collision in a hash table?\",\n",
    "    \"How can you implement a load balancer for a distributed system?\",\n",
    "    \"What is the fable involving a fox and grapes?\",\n",
    "    \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
    "    \"Who does Harry turn into a balloon?\",\n",
    "    \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
    "    \"Describe a day in the life of a secret agent who's also a full-time parent.\",\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    \"El planeta Tierra es \",\n",
    "    \"Implementar una función en Python para calcular los números de Fibonacci.\",\n",
    "    \"Escribir una función en Rust que realice una exponenciación binaria.\",\n",
    "    \"¿Cómo reservo memoria en C?\",\n",
    "    \"¿Cuáles son las diferencias entre Javascript y Python?\",\n",
    "    \"¿Cómo encuentro índices inválidos en Postgres?\",\n",
    "    \"¿Cómo puedes implementar una caché LRU (Least Recently Used) en Python?\",\n",
    "    \"¿Qué enfoque usarías para detectar y prevenir condiciones de carrera en una aplicación multiproceso?\",\n",
    "    \"¿Puedes explicar cómo funciona un algoritmo de árbol de decisión en el aprendizaje automático?\",\n",
    "    \"¿Cómo diseñarías una base de datos simple de almacenamiento clave-valor desde cero?\",\n",
    "    \"¿Cómo manejas situaciones de interbloqueo en la programación concurrente?\",\n",
    "    \"¿Cuál es la lógica detrás del algoritmo de búsqueda A*, y dónde se utiliza?\",\n",
    "    \"¿Cómo puedes diseñar un sistema de autocompletado eficiente?\",\n",
    "    \"¿Qué enfoque tomarías para diseñar un sistema seguro de gestión de sesiones en una aplicación web?\",\n",
    "    \"¿Cómo manejarías las colisiones en una tabla hash?\",\n",
    "    \"¿Cómo puedes implementar un balanceador de carga para un sistema distribuido?\",\n",
    "    \"Escribe un cuento sobre un historiador que viaja en el tiempo y que presencie los eventos más significativos de la historia de la humanidad.\",\n",
    "    \"Describe un día en la vida de un agente secreto que también es padre/madre a tiempo completo.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm(queries):\n",
    "    t0 = time.perf_counter()\n",
    "    res = call_llms(config, q, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"llm-comparison\": table})\n",
    "table = wandb.Table(columns=[\"model\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
