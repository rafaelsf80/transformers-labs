{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05-4. Comparativa LLMs y métricas en wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install openai\n",
    "!pip install cohere\n",
    "!pip install google-cloud-platform --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "config = dict(\n",
    "    temperature = 1.0,\n",
    "    max_output_tokens = 128,\n",
    "    top_p = 0.8,\n",
    "    top_k = 40,\n",
    ")\n",
    "wandb.init(project=\"comparison-openai-google\", config=config, name = \"comparison\")\n",
    "print(wandb.util.generate_id())\n",
    "print(wandb.run)\n",
    "table = wandb.Table(columns=[\"model\", \"test\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GCP and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change PROJECT_ID\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\"   # <---- CHANGE THIS\n",
    "LOCATION = \"us-central1\"   \n",
    "# Code examples may misbehave if the model is changed.\n",
    "MODEL_NAME = \"text-bison@001\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "\n",
    "google_palm_model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "google_gemini_model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Change OPENAI API KEY\n",
    "openai_client = OpenAI(api_key=YOUR_OPENAI_API_KEY)  # <---- CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(YOUR_COHERE_API_KEY)  # <---- CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai\n",
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY='AIzaSyC93QZK9ZJivQf1LFisn0aKje5WK1FMwMc'\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "google_gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_llms(description, parameters, llm_call, show_activity = True):\n",
    "  \n",
    " # text-bison\n",
    "  t0 = time.perf_counter()\n",
    "  res_google = google_palm_model.predict(llm_call, **parameters).text\n",
    "  table.add_data(\n",
    "      \"text-bison@001\", \n",
    "      description, \n",
    "      time.perf_counter() - t0, \n",
    "      config[\"temperature\"], \n",
    "      config[\"max_output_tokens\"], \n",
    "      config[\"top_p\"], \n",
    "      config[\"top_k\"], \n",
    "      llm_call, \n",
    "      res_google)\n",
    "  \n",
    "  # gpt-3.5-turbo (ChatGPT)\n",
    "  t0 = time.perf_counter()\n",
    "  res_openai = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )  \n",
    "  table.add_data(\n",
    "    \"gpt-3.5-turbo\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "  \n",
    "  # gpt-4-8k\n",
    "  t0 = time.perf_counter()\n",
    "  res_openai = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": llm_call},\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  table.add_data(\n",
    "    \"gpt-4-0613\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    config[\"temperature\"], \n",
    "    config[\"max_output_tokens\"], \n",
    "    config[\"top_p\"], \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_openai.choices[0].message.content\n",
    "  )\n",
    "\n",
    "  # Gemini text\n",
    "  t0 = time.perf_counter() \n",
    "  res_google = google_gemini_model.generate_content(\n",
    "    [llm_call],\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 1\n",
    "    },\n",
    "  stream=False,\n",
    "  ) \n",
    "  table.add_data(\n",
    "    \"gemini-pro\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    0.9, \n",
    "    2048, \n",
    "    1, \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_google.text\n",
    "  )\n",
    "  \n",
    "  # Cohere\n",
    "  t0 = time.perf_counter() \n",
    "  res_cohere = co.generate(  \n",
    "    model='command-nightly',  \n",
    "    prompt = llm_call,  \n",
    "    max_tokens=200, # This parameter is optional. \n",
    "    temperature=0.750)\n",
    "  table.add_data(\n",
    "    \"cohere\", \n",
    "    description, \n",
    "    time.perf_counter() - t0, \n",
    "    0.750, \n",
    "    200, \n",
    "    1, \n",
    "    config[\"top_k\"], \n",
    "    llm_call, \n",
    "    res_cohere.generations[0].text  \n",
    "  )\n",
    "    \n",
    "  ## Only show response from Gemini-pro, not from all LLMs\n",
    "  if show_activity:\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
    "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
    "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
    "    print(res_google)\n",
    "        \n",
    "\n",
    "  return res_google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"El planeta Tierra es \",\n",
    "    \"Implementar una función en Python para calcular los números de Fibonacci.\",\n",
    "    \"Escribir una función en Rust que realice una exponenciación binaria.\",\n",
    "    \"¿Cómo reservo memoria en C?\",\n",
    "    \"¿Cuáles son las diferencias entre Javascript y Python?\",\n",
    "    \"¿Cómo encuentro índices inválidos en Postgres?\",\n",
    "    \"¿Cómo puedes implementar una caché LRU (Least Recently Used) en Python?\",\n",
    "    \"¿Qué enfoque usarías para detectar y prevenir condiciones de carrera en una aplicación multiproceso?\",\n",
    "    \"¿Puedes explicar cómo funciona un algoritmo de árbol de decisión en el aprendizaje automático?\",\n",
    "    \"¿Cómo diseñarías una base de datos simple de almacenamiento clave-valor desde cero?\",\n",
    "    \"¿Cómo manejas situaciones de interbloqueo en la programación concurrente?\",\n",
    "    \"¿Cuál es la lógica detrás del algoritmo de búsqueda A*, y dónde se utiliza?\",\n",
    "    \"¿Cómo puedes diseñar un sistema de autocompletado eficiente?\",\n",
    "    \"¿Qué enfoque tomarías para diseñar un sistema seguro de gestión de sesiones en una aplicación web?\",\n",
    "    \"¿Cómo manejarías las colisiones en una tabla hash?\",\n",
    "    \"¿Cómo puedes implementar un balanceador de carga para un sistema distribuido?\",\n",
    "    \"Escribe un cuento sobre un historiador que viaja en el tiempo y que presencie los eventos más significativos de la historia de la humanidad.\",\n",
    "    \"Describe un día en la vida de un agente secreto que también es padre/madre a tiempo completo.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in tqdm(queries):\n",
    "    t0 = time.perf_counter()\n",
    "    res = call_llms(\"comparison\", parameters, q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"comparison\": table})\n",
    "table = wandb.Table(columns=[\"model\", \"test\", \"time\", \"temperature\", \"max_output_tokens\", \"top_p\", \"top_k\", \"prompt\", \"response\"])\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
