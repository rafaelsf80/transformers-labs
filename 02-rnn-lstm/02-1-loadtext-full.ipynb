{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-1: Load text - Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset from tensorflow\n",
    "data_url = 'https://storage.googleapis.com/miax/stack_overflow_16k.tar.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do 80:20 on the train_set split using `tf.keras.utils.text_dataset_from_directory` with `validation_split` set to  `0.2` (i.e. 20%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de .cache y .prefetch para mejorar el performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement .cache and .prefect on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar dataset para el entrenamiento\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement binary vectorization (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement integer vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = tf.keras.Sequential(\n",
    "    # TODO: Capas modelo  para datos vectorizados binarios (bag-of-words linear model)\n",
    "    )\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tf.keras.utils.plot_model(binary_model, show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_history = binary_model.fit(\n",
    "    raw_train_ds, validation_data=raw_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL LSTM\n",
    "##Epoch 10/10\n",
    "##200/200 [==============================] - 32s 158ms/step - loss: 0.1786 - accuracy: 0.9377 - val_loss: 0.7592 - val_accuracy: 0.7588\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  # TODO: Capas Modelo LSTM\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model\n",
    "\n",
    "\n",
    "## MODEL BIDIRECTIONAL\n",
    "##Epoch 10/10\n",
    "##200/200 [==============================] - 58s 290ms/step - loss: 0.0869 - accuracy: 0.9736 - val_loss: 0.8587 - val_accuracy: 0.7519\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  # TODO: Capas Modelo LSTM bidireccional\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model\n",
    "\n",
    "## MODEL DNN\n",
    "##Epoch 10/10\n",
    "##200/200 [==============================] - 1s 5ms/step - loss: 0.4998 - accuracy: 0.8423 - val_loss: 0.5464 - val_accuracy: 0.8313\n",
    "\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [binary_vectorize_layer]\n",
    "\n",
    "  # TODO: Capas Modelo DNN\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4, vectorizer=int_vectorize_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(int_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "int_history = int_model.fit(raw_train_ds, validation_data=raw_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = plt.plot(bin_history.epoch, bin_history.history['loss'], label='bin-loss')\n",
    "plt.plot(bin_history.epoch, bin_history.history['val_loss'], '--', color=loss[0].get_color(), label='bin-val_loss')\n",
    "\n",
    "loss = plt.plot(int_history.epoch, int_history.history['loss'], label='int-loss')\n",
    "plt.plot(int_history.epoch, int_history.history['val_loss'], '--', color=loss[0].get_color(), label='int-val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.export('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.predict(['How do you sort a list?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded.serve(tf.constant(['How do you sort a list?'])).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
