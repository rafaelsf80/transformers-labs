{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-6: ScaNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scann pandas seaborn \n",
    "!pip install tensorflow==2.11.0 # Required for compatibility with tf_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-samples-data/vertex-ai/dataset-management/datasets/bert_finetuning/wide_and_deep_trainer_container_tests_input.jsonl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scann\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)\n",
    "\n",
    "\n",
    "records = []\n",
    "with open(\"wide_and_deep_trainer_container_tests_input.jsonl\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        records.append(record)\n",
    "\n",
    "\n",
    "# Peek at the data.\n",
    "df = pd.DataFrame(records)\n",
    "print(df.head(50))\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "  #message_embeddings = embed(messages)\n",
    "\n",
    "  #for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    #print(\"Message: {}\".format(messages[i]))\n",
    "    #print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    #message_embedding_snippet = \", \".join(\n",
    "    #    (str(x) for x in message_embedding[:3]))\n",
    "    #print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "\n",
    "  return embed([text])\n",
    "\n",
    "import tensorflow as tf\n",
    "# This may take several minutes to complete.\n",
    "df[\"embedding\"] = df[\"textContent\"].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear indice ScaNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Index\n",
    "record_count = len(records)\n",
    "print(\"recor_cont\")\n",
    "print(record_count)\n",
    "\n",
    "dataset = np.empty((record_count, 512)) # embedding size of sentence encoder (768 if palm)\n",
    "for i in range(record_count):\n",
    "    dataset[i] = df.embedding[i]\n",
    "\n",
    "normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "\n",
    "# use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher\n",
    "searcher = (\n",
    "    scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\")\n",
    "    .tree(\n",
    "        num_leaves=record_count,\n",
    "        num_leaves_to_search=record_count,\n",
    "        training_sample_size=record_count,\n",
    "    )\n",
    "    .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "    .reorder(100)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanzar queries al index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    start = time.time()\n",
    "    query = get_embedding(query)\n",
    "    print(query)\n",
    "    neighbors, distances = searcher.search(tf.reshape(query,[-1]), final_num_neighbors=3)\n",
    "    end = time.time()\n",
    "\n",
    "    for id, dist in zip(neighbors, distances):\n",
    "        print(f\"[docid:{id}] [{dist}] -- {df.textContent[int(id)][:125]}...\")\n",
    "    print(\"Latency (ms):\", 1000 * (end - start))\n",
    "\n",
    "\n",
    "search(\"tell me about shark or animal\")\n",
    "\n",
    "\n",
    "search(\"tell me about an important moment or event in your life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n con seaborn (semantic search similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embed(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)\n",
    "\n",
    "\n",
    "# TODO: Write messages to compare, at least 10 messages\n",
    "\n",
    "messages = []\n",
    "\n",
    "run_and_plot(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
