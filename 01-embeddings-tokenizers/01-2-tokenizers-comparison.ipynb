{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-2: Tokenizers. Subpalabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro: Unicode and UTF-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Spanish and SPANISH ðŸ‘‹\n",
    "    espaÃ±ol\n",
    "    ESPAÃ‘OL.\n",
    "    \n",
    "    ðŸ˜€ðŸŽµ\n",
    "    show_tokens False None elif == >= else\n",
    "    Two tabs: \"\\t\\t\" Four spaces: \"    \"\n",
    "    12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Obtener caracteres Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El tokenizador MinBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/minbpe.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usar MinBPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El tokenizador Tiktoken y otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name, print_token_ids = False):\n",
    "    if (tokenizer_name == 'gpt-4'):\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        token_ids = tokenizer.encode(sentence)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        print(len(tokenizer))\n",
    "        token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    for t in token_ids:\n",
    "        if print_token_ids:\n",
    "            print(t, end = \" \")\n",
    "        print('\\x1b[0;30;47m' + tokenizer.decode([t]) +'\\x1b[0m', end = \" \")\n",
    "\n",
    "    print('\\n\\n')\n",
    "\n",
    "    for t in token_ids:\n",
    "        print(t, '\\x1b[0;30;47m' + tokenizer.decode([t]) +'\\x1b[0m')\n",
    "    \n",
    "\n",
    "def encode_decode(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    print(tokenizer.decode(token_ids))\n",
    "\n",
    "text = \"\"\"\n",
    "    English and CAPITALIZATION\n",
    "    ðŸ˜€ðŸŽµ\n",
    "    show_tokens False None elif == >= else\n",
    "    Two tabs: \"\\t\\t\" Four spaces: \"    \"\n",
    "    12.0*50=600\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test and compare bert-base-uncased,bert-base-cased, GPT2, T5-small, GPT-4 tokenizers, bigcode/starcoder, facebook/galactica-1.3b\n",
    "# NOTE: bigcode/starcoder requires huggingface login and accept T&Cs at starcoder page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El tokenizador SentencePiece. Entrenamiento y uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training options: https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization --- IMPORTANT, PERO COMO DATA SCIENTISTS MEJOR MANTENER EL RAW DATA\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True, # IMPORTANTE: RESERVA 0X A 0X256\n",
    "  # merge rules - IGUAL QUE LAS REGULAR EXPRESSIONS DE GPT-2\n",
    "  split_digits=True,  # split digits\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1, # no usamos token de padding\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cargar tokenizdor anterior y ver vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Probar tokenizer anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public open sourced pre-trained weights are available here (32k tokens): gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model.\n",
    "# You must download the weights to run the samples in this snippet (32K tokens):\n",
    "# `gsutil cp gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Probar tokenizer entrenado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
